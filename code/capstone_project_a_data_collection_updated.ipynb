{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: Books recommender system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is always a need for book recommendation when one would like to start picking a new book to read. With the vast availability of book choices we have currently, readers always faced with the difficulty of choosing a book to read and on average take a huge amount of time, effort and energy to select and choose a book to read. This include social media, online browsing and book referrals, which is time-consuming [[1]](https://www.nlb.gov.sg/Portals/0/Docs/AboutUs/2018%20NATIONAL%20READING%20HABITS%20STUDY%20ON%20ADULTS%20-%20REPORT.pdf). With the current lacking of a book recommendation system in libraries, as a data scientist in the national library, we aim to help reader to save time to decide for which books to read, by developing book recommendations system which matches their  preferences. The team will explore the use of popularity-based system, content-based system and collaborative filtering-based system for predicting the books that the readers would rate. The model's success will be evaluated based on the root mean square error that will evaluate the differences between the actual rating and the predicted ratings a reader will rate for a book. The closer the predicted rating to the actual rating, the better the book can be selected for recommendation,which will enhance the reader experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Contents:\n",
    "- [Background](#1.-Background) **(In this notebook)**\n",
    "- [Data Collection](#2.-Data-Collection) **(In this notebook)**\n",
    "- Data Cleaning Booklist\n",
    "- Data Cleaning Book Interactions\n",
    "- Exploratory Data Analysis\n",
    "- Modeling 1 Popularity-based system\n",
    "- Modeling 2 Content-based system\n",
    "- Modeling 3 Collaborative-based system\n",
    "- Evaluation\n",
    "- Conclusion and Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Datasets\n",
    "\n",
    "As goodreads has one of the world's largest site for readers and book recommendations, we will be using the goodreads datasets obtained from [University of California San Diego Book Graph](https://sites.google.com/eng.ucsd.edu/ucsdbookgraph/home?authuser=0) to aid in the development of book recommendation system [1]. \n",
    "The assumption is goodread readers are random and come from all around the world, in which the data is symbolic of readers in national libraries.\n",
    "The dataset contains meta-data of books and user-book interactions.\n",
    "\n",
    "The datasets obtained are as followed:-\n",
    "\n",
    "Meta-data of books:-\n",
    "* goodreads_books \n",
    "* goodreads_book_authors\n",
    "* goodreads_book_series\n",
    "* goodreads_book_genres_initial\n",
    "\n",
    "User-book interactions:-\n",
    "* goodreads_interactions\n",
    "* book_id_map\n",
    "\n",
    "For more details, please refer to the data_dictionary.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pandas pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview**\n",
    "\n",
    "Goodreads_books.json.gz file is stored in a gzip file format and has a size of 2GB. The file could not be load directly and therefore the file is extracted in batches and saved in subfiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Define function for data collection from json.gz (large files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# book_total_line = sum(1 for line in gzip.open('./datasets/goodreads_books.json.gz'))\n",
    "# print(f\"Total line in goodreads book json is : {book_total_line}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((book_total_line/5)%5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collection_json(filepath, min_count, max_count):\n",
    "    count = 0\n",
    "    datalist = []\n",
    "    \n",
    "    with gzip.open(filepath) as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            count +=1\n",
    "            \n",
    "            if (count>min_count) & (count<=(max_count)):\n",
    "                datalist.append(data)\n",
    "                clear_output(wait=True)\n",
    "                print(f'progress: {count}/{max_count}')\n",
    "\n",
    "            elif count>(max_count):\n",
    "                break\n",
    "        \n",
    "    return datalist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Collection from goodreads_books.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#booklist_first = data_collection_json('./datasets/goodreads_books.json.gz', 0, (book_total_line/5))\n",
    "#booklist_second = data_collection_json('./datasets/goodreads_books.json.gz', (book_total_line/5), ((book_total_line/5)*2))\n",
    "#booklist_third = data_collection_json('./datasets/goodreads_books.json.gz', ((book_total_line/5)*2), ((book_total_line/5)*3))\n",
    "#booklist_fourth = data_collection_json('./datasets/goodreads_books.json.gz', ((book_total_line/5)*3), ((book_total_line/5)*4))\n",
    "#booklist_fifth = data_collection_json('./datasets/goodreads_books.json.gz', ((book_total_line/5)*4), (((book_total_line/5)*5)+100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#booklist_first = pd.DataFrame(booklist_first)\n",
    "#booklist_second = pd.DataFrame(booklist_second)\n",
    "#booklist_third = pd.DataFrame(booklist_third)\n",
    "#booklist_fourth = pd.DataFrame(booklist_fourth)\n",
    "#booklist_fifth = pd.DataFrame(booklist_fifth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data exportation into other formats\n",
    "\n",
    "Parquet file format is a format that allows to process large data with small file size. For ease of data analysis of different datasets, the file will be changed into parquet file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "booklist_authors = pd.read_json(\"./datasets/goodreads_book_authors.json.gz\", lines = True)\n",
    "booklist_series = pd.read_json(\"./datasets/goodreads_book_series.json.gz\", lines = True)\n",
    "booklist_works = pd.read_json(\"./datasets/goodreads_book_works.json.gz\", lines = True)\n",
    "booklist_genres = pd.read_json(\"./datasets/goodreads_book_genres_initial.json.gz\", lines = True)\n",
    "booklist_interactions = pd.read_csv(\"./datasets/goodreads_interactions.csv\")\n",
    "book_id_map = pd.read_csv(\"./datasets/book_id_map.csv\")\n",
    "user_id_map = pd.read_csv(\"./datasets/user_id_map.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The goodreads_books.json has successfully collected and separated into five subfiles. These files will be exported as a parquet files.\n",
    "* The other documents have been imported and will be exported as a parquet file to be cleaned in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From goodreads_books.json.gz**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Placed the # to refrain from executing\n",
    "# booklist_first.to_csv(\"./data/booklist_first.csv\", index = False) \n",
    "# booklist_second.to_csv(\"./data/booklist_second.csv\", index = False) \n",
    "# booklist_third.to_csv(\"./data/booklist_third.csv\", index = False) \n",
    "# booklist_fourth.to_csv(\"./data/booklist_fourth.csv\", index = False) \n",
    "#booklist_fifth.to_csv(\"./data/booklist_fifth.csv\", index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#booklist_first.to_parquet('./data/booklist_first.parquet', compression='gzip')\n",
    "#booklist_second.to_parquet('./data/booklist_second.parquet', compression='gzip')\n",
    "#booklist_third.to_parquet('./data/booklist_third.parquet', compression='gzip')\n",
    "#booklist_fourth.to_parquet('./data/booklist_fourth.parquet', compression='gzip')\n",
    "#booklist_fifth.to_parquet('./data/booklist_fifth.parquet', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From goodreads_books.json.gz**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "booklist_authors.to_parquet('./data/booklist_authors.parquet', compression='gzip')\n",
    "booklist_series.to_parquet('./data/booklist_series.parquet', compression='gzip')\n",
    "booklist_works.to_parquet('./data/booklist_works.parquet', compression='gzip')\n",
    "booklist_genres.to_parquet('./data/booklist_genres.parquet', compression='gzip')\n",
    "booklist_interactions.to_parquet('./data/booklist_interactions.parquet', compression='gzip')\n",
    "book_id_map.to_parquet('./data/book_id_map.parquet', compression='gzip')\n",
    "user_id_map.to_parquet('./data/user_id_map.parquet', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] M. Wan, and J. McAuley, \"Item Recommendation on Monotonic Behvaior Chains,\" *Proceedings of the 12th ACM conference on Recommender Systems, RecSys 2018*, September 2018, pp. 86-94. doi: 10.1145/3240323.3240369 [Online]. Available:https://dl.acm.org/doi/10.1145/3240323.3240369 [Accessed: May 10, 2021]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
